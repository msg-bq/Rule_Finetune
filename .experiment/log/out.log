2024-02-04 17:53:49,616 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:85] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=10, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-04 17:54:14,649 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:85] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=10, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-04 17:54:14,685 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-04 17:55:37,250 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6024444231891043
2024-02-04 17:57:24,188 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6945238365970077
2024-02-04 17:59:07,669 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.7159410862769072
2024-02-04 18:00:41,819 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.7028720579140747
2024-02-04 18:02:13,104 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.670568680428306
2024-02-04 18:03:32,816 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch5的平均score为：0.6984947312660295
2024-02-04 18:04:56,784 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch6的平均score为：0.7193168159965037
2024-02-04 18:06:27,635 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch7的平均score为：0.6443710661571195
2024-02-04 18:08:31,121 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch8的平均score为：0.6979898356851482
2024-02-04 18:10:08,501 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch9的平均score为：0.7012010954924341
2024-02-05 10:50:22,064 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:85] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 10:50:22,097 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 11:34:14,877 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:85] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 11:34:14,892 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 11:35:27,035 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6126724466010182
2024-02-05 11:36:33,974 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.651042533855034
2024-02-05 11:37:43,390 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.7431265476378259
2024-02-05 11:46:10,576 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 11:46:15,613 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 11:48:34,124 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6435753532182108
2024-02-05 11:51:20,764 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 11:51:27,860 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 11:52:29,687 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6381344845630561
2024-02-05 12:06:07,356 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 12:06:13,849 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 12:09:44,742 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6391302550877023
2024-02-05 12:10:35,219 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6121817696787099
2024-02-05 12:11:28,611 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.6386053886053891
2024-02-05 12:17:41,919 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=3, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 12:17:51,120 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 12:19:08,482 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6342472418292093
2024-02-05 12:22:48,175 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6938679489908998
2024-02-05 12:23:47,384 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.6447583477128933
2024-02-05 12:29:21,477 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 12:29:25,895 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 12:30:32,991 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6561053013180675
2024-02-05 12:31:29,790 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6510963591116266
2024-02-05 12:32:26,808 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.6919795890384125
2024-02-05 12:33:29,810 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.6912191818206856
2024-02-05 12:34:32,102 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.688155681853161
2024-02-05 12:46:34,877 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 12:46:40,265 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 12:47:49,927 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6103706245792576
2024-02-05 12:48:57,969 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6719201224840323
2024-02-05 12:49:52,436 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.6644764186430856
2024-02-05 12:50:51,396 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.6958544842691187
2024-02-05 12:51:49,787 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.6514157454553087
2024-02-05 14:57:29,938 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 14:59:51,995 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 15:03:56,497 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 15:13:28,279 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 15:33:25,337 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 15:36:35,095 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 15:40:30,882 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 15:45:51,875 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.675488774828951
2024-02-05 15:49:24,616 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6874385014669373
2024-02-05 15:54:25,279 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.7079121038451185
2024-02-05 16:00:21,802 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.7086648799673606
2024-02-05 16:06:18,160 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.7248287647312037
2024-02-05 18:04:36,200 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 18:05:05,925 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 18:05:45,751 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 18:08:40,682 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 18:08:44,579 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 18:13:03,012 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6979634240547944
2024-02-05 18:19:04,025 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6990788571985148
2024-02-05 18:23:16,257 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.6921861144473502
2024-02-05 18:28:49,596 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.6931274424808908
2024-02-05 18:32:35,664 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.6818788886343599
2024-02-05 18:50:17,665 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 18:50:25,713 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 18:53:39,884 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.7103225707766366
2024-02-05 19:11:06,983 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 19:11:15,244 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-05 19:15:56,050 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch0的平均score为：0.6986788884946508
2024-02-05 19:19:12,200 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch1的平均score为：0.6930714851650813
2024-02-05 19:23:26,295 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch2的平均score为：0.682766223173778
2024-02-05 19:27:38,226 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch3的平均score为：0.6896499502521775
2024-02-05 19:31:56,536 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:158] - INFO: epoch4的平均score为：0.6765532657786878
2024-02-05 21:18:52,807 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:19:20,098 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:27:20,583 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:30:59,419 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:45:56,775 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:49:41,198 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-05 21:50:00,611 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:86] - INFO: args: Namespace(cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir=None, dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True)
2024-02-06 18:17:58,973 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:20:22,529 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:20:33,712 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:22:00,551 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:22:05,284 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:23:13,718 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:23:21,908 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:25:53,138 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:25:59,626 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:27:17,627 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:27:23,128 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:34:19,843 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:34:27,390 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 18:39:20,227 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:51:36,420 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:54:48,867 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:55:48,760 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:56:56,604 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 18:57:00,848 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:00:28,229 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:00:34,120 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:03:32,297 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:08:16,366 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:08:20,285 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:09:33,123 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Context: The relations on the path from Alan to Anthony are daughter, uncle, son.\nQuestion: Anthony is Alan’s what?\nAnswer:\nFor daughter’s uncle, we have daughter’s uncle is brother. So the relations are reduced to brother, son.\nFor brother’s son, we have brother’s son is nephew. So the relations are reduced to nephew.\nTherefore, the answer is nephew.\n\nContext: The relations on the path from Annie to Carlos are brother, mother, son.\nQuestion: Carlos is Annie’s what?\nAnswer:\nFor brother’s mother, we have brother’s mother is mother. So the relations are reduced to mother, son.\nFor mother’s son, we have mother’s son is brother. So the relations are reduced to brother.\nTherefore, the answer is brother.\n\nContext: The relations on the path from Beverly to Michelle are father, daughter, aunt.\nQuestion: Michelle is Beverly’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, aunt.\nFor sister’s aunt, we have sister’s aunt is aunt. So the relations are reduced to aunt.\nTherefore, the answer is aunt.\n\nContext: The relations on the path from Lee to Jeanna are father, daughter, sister.\nQuestion: Jeanna is Lee’s what?\nAnswer:\nFor father’s daughter, we have father’s daughter is sister. So the relations are reduced to sister, sister.\nFor sister’s sister, we have sister’s sister is sister. So the relations are reduced to sister.\nTherefore, the answer is sister.\n\nContext: The relations on the path from Craig to Molly are sister, father, mother.\nQuestion: Molly is Craig’s what?\nAnswer:\nFor sister’s father, we have sister’s father is father. So the relations are reduced to father, mother.\nFor father’s mother, we have father’s mother is grandmother. So the relations are reduced to grandmother.\nTherefore, the answer is grandmother.', cot_trigger_type='HtT_version', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:09:39,051 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:12:03,479 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:12:07,315 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:14:14,824 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:14:20,717 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:16:00,006 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:16:05,928 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:17:11,000 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:17:16,746 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:17:31,879 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:17:36,171 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:18:16,576 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/CLUTRR', dataset='CLUTRR', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The answer is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The answer is', random_seed=192, rationale_dir=None, save_dir='./experiment', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:19:01,144 - C:/Users/lbq/Documents/GitHub/Rule_Finetune/main.py[line:109] - INFO: args: Namespace(cold_start_num=200, cot_trigger='Answer: Let\'s think step by step. First rationale then answer.If you use any prior knowledge or rule during the inference, write them briefly in "<Begin>xxx</End>" format. Only do this if you find them. Note that these rules should be true in general.', cot_trigger_type='default1', data_dir='./data/LANG_8', dataset='LANG_8', debug=True, demo_save_dir='demosave', direct_answer_trigger_for_zeroshot_cot='The revised grammatically correct sentence is', encoder='all-MiniLM-L6-v2', epoch=5, eval=False, llm_model='gpt-3.5-turbo-1106', llm_model_path=None, multi_thread=True, num_clusters=5, pred_trigger='The revised grammatically correct sentence is', random_seed=192, rationale_dir=None, save_dir='./experiment/LANG_8/version_0', test=False, topN=1, train=True, train_dataset_size=200, train_prompt='Instruction: Following are several existed knowledge in knowledge base. When you answer the questions, try to use the provided knowledge whenever possible in <retrieved_rule>xxx<retrieved_rule> format. Try not to invent knowledge by yourself unless necessary. But if so, you are permitted to establish your own rules in <new_rule>xxx<new_rule> format.\n', train_prompt_type='default1')
2024-02-06 19:21:08,741 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:42] - INFO: 完成cold start
2024-02-06 19:23:10,612 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:145] - INFO: epoch0的平均score为：0.7942420613455372
2024-02-06 19:24:48,315 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:145] - INFO: epoch1的平均score为：0.8207560061310593
2024-02-06 19:26:37,185 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:145] - INFO: epoch2的平均score为：0.7805511205705008
2024-02-06 19:28:38,011 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:145] - INFO: epoch3的平均score为：0.7915043033235398
2024-02-06 19:30:29,661 - C:\Users\lbq\Documents\GitHub\Rule_Finetune\RuleFinetune\RuleTrainer.py[line:145] - INFO: epoch4的平均score为：0.7704001152960256
